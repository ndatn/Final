{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.1.18:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [10/Jun/2024 21:38:03] \"OPTIONS /api/answer HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [10/Jun/2024 21:38:20] \"POST /api/answer HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [10/Jun/2024 21:38:43] \"OPTIONS /api/answer HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [10/Jun/2024 21:38:53] \"OPTIONS /api/answer HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [10/Jun/2024 21:38:53] \"POST /api/answer HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [10/Jun/2024 21:39:03] \"POST /api/answer HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [10/Jun/2024 21:41:12] \"OPTIONS /api/answer HTTP/1.1\" 200 -\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "127.0.0.1 - - [10/Jun/2024 21:41:25] \"OPTIONS /api/answer HTTP/1.1\" 200 -\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "127.0.0.1 - - [10/Jun/2024 21:41:58] \"POST /api/answer HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [10/Jun/2024 21:42:00] \"POST /api/answer HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [10/Jun/2024 21:42:12] \"POST /api/answer HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [10/Jun/2024 21:42:26] \"POST /api/answer HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from flask_cors import CORS\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def summarize_text(text, max_length=150):\n",
    "    inputs = tokenizer([text], max_length=1024, return_tensors=\"pt\", truncation=True)\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=max_length, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    return summary\n",
    "\n",
    "@app.route(\"/api/answer\", methods=[\"POST\"])\n",
    "def get_answer():\n",
    "    data = request.get_json()\n",
    "    text = data.get(\"text\", \"\")\n",
    "    question = data.get(\"question\", \"\")\n",
    "\n",
    "    if question:\n",
    "        inputs = tokenizer(question, text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, num_beams=4, max_length=128, early_stopping=True)\n",
    "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        return jsonify({\"question\": question, \"answer\": answer})\n",
    "    else:\n",
    "        summary = summarize_text(text)\n",
    "        return jsonify({\"summary\": summary})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0', port=5000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
